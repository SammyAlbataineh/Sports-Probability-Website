{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP3blj/6X1pODjX74gM4krd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SammyAlbataineh/Sports-Probability-Website/blob/main/NFL_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5YFa1OKDcFP"
      },
      "outputs": [],
      "source": [
        "!pip install kagglehub transformers timm -q\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"tobycrabtree/nfl-scores-and-betting-data\")\n",
        "print(\"Downloaded path:\", path)"
      ],
      "metadata": {
        "id": "hWKZ4dNZMqjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abbb57c2",
        "collapsed": true
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Construct the full path to the desired CSV file\n",
        "scores_csv_path = os.path.join(path, 'spreadspoke_scores.csv')\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "df_scores = pd.read_csv(scores_csv_path)\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "display(df_scores.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3579d997"
      },
      "source": [
        "df_scores = df_scores.drop(columns=['weather_detail'])\n",
        "display(df_scores.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e6258af"
      },
      "source": [
        "print(path)\n",
        "for root, dirs, files in os.walk(path):\n",
        "    for name in files:\n",
        "        print(os.path.join(root, name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores['home_win'] = (df_scores['score_home'] > df_scores['score_away']).astype(int)\n",
        "y = df_scores[['score_home', 'score_away', 'home_win']]\n",
        "x = df_scores.drop(columns=['score_home', 'score_away', 'home_win'])\n",
        "display(df_scores.head())"
      ],
      "metadata": {
        "id": "QFCJMw0GYcAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "x_encoded = encoder.fit_transform(x[x.columns])"
      ],
      "metadata": {
        "id": "sH4fv97mYgY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efd9fa39"
      },
      "source": [
        "print(df_scores.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b15a8547"
      },
      "source": [
        "median_score_home = df_scores['score_home'].median()\n",
        "median_score_away = df_scores['score_away'].median()\n",
        "\n",
        "df_scores['score_home'].fillna(median_score_home, inplace=True)\n",
        "df_scores['score_away'].fillna(median_score_away, inplace=True)\n",
        "\n",
        "print(f\"Median score home: {median_score_home}\")\n",
        "print(f\"Median score away: {median_score_away}\")\n",
        "print(\"Missing values after filling:\")\n",
        "print(df_scores[['score_home', 'score_away']].isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa7eac2d"
      },
      "source": [
        "median_score_home = df_scores['score_home'].median()\n",
        "median_score_away = df_scores['score_away'].median()\n",
        "\n",
        "df_scores['score_home'] = df_scores['score_home'].fillna(median_score_home)\n",
        "df_scores['score_away'] = df_scores['score_away'].fillna(median_score_away)\n",
        "\n",
        "print(f\"Median score home: {median_score_home}\")\n",
        "print(f\"Median score away: {median_score_away}\")\n",
        "print(\"Missing values after filling:\")\n",
        "print(df_scores[['score_home', 'score_away']].isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffd36b6f"
      },
      "source": [
        "print(\"Missing values in 'score_home' and 'score_away' after imputation:\")\n",
        "print(df_scores[['score_home', 'score_away']].isnull().sum())\n",
        "\n",
        "print(\"\\nMissing values in the entire df_scores DataFrame:\")\n",
        "print(df_scores.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5265571f"
      },
      "source": [
        "display(df_scores.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f760702"
      },
      "source": [
        "y_target = y['home_win']\n",
        "X = x_encoded\n",
        "\n",
        "print(\"Shape of y_target:\", y_target.shape)\n",
        "print(\"Shape of X:\", X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b03a3ef9"
      },
      "source": [
        "latest_season = df_scores['schedule_season'].max()\n",
        "weights = (df_scores['schedule_season'] - df_scores['schedule_season'].min()) / \\\n",
        "          (latest_season - df_scores['schedule_season'].min()) + 0.1 # Add a small constant to ensure non-zero weights\n",
        "\n",
        "print(\"First 5 calculated weights:\")\n",
        "print(weights.head())\n",
        "print(\"Last 5 calculated weights:\")\n",
        "print(weights.tail())\n",
        "print(\"Shape of weights:\", weights.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e6cc8d1"
      },
      "source": [
        "cutoff_season = 2023\n",
        "\n",
        "train_mask = df_scores['schedule_season'] <= cutoff_season\n",
        "test_mask = df_scores['schedule_season'] > cutoff_season\n",
        "\n",
        "X_train, X_test = X[train_mask], X[test_mask]\n",
        "y_train, y_test = y_target[train_mask], y_target[test_mask]\n",
        "weights_train, weights_test = weights[train_mask], weights[test_mask]\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
        "print(f\"Max season in training set: {df_scores[train_mask]['schedule_season'].max()}\")\n",
        "print(f\"Min season in test set: {df_scores[test_mask]['schedule_season'].min()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d12c429"
      },
      "source": [
        "cutoff_season = 2023\n",
        "\n",
        "train_mask = df_scores['schedule_season'] <= cutoff_season\n",
        "test_mask = df_scores['schedule_season'] > cutoff_season\n",
        "\n",
        "X_train, X_test = X[train_mask], X[test_mask]\n",
        "y_train, y_test = y_target[train_mask], y_target[test_mask]\n",
        "weights_train, weights_test = weights[train_mask], weights[test_mask]\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
        "print(f\"Max season in training set: {df_scores[train_mask]['schedule_season'].max()}\")\n",
        "print(f\"Min season in test set: {df_scores[test_mask]['schedule_season'].min()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d2b3c80"
      },
      "source": [
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")\n",
        "print(f\"Shape of weights_train: {weights_train.shape}\")\n",
        "print(f\"Shape of weights_test: {weights_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9889de80"
      },
      "source": [
        "model = LogisticRegression(solver='liblinear', random_state=42, max_iter=2000)\n",
        "model.fit(X_train, y_train, sample_weight=weights_train)\n",
        "\n",
        "print(\"Logistic Regression model successfully fitted to the training data with time-decaying weights.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy on Test Set: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "WrdTLQjhyPuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y_BbaF1VySTG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}